{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ritwiks9635/Image_Captioning/blob/main/Image_Captioning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57J74Ro8y2vP"
      },
      "source": [
        "#**ðŸŽ‘ImageðŸŒ‰ðŸ’¬CaptioningðŸ”¡**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCDxf1XWzyF9"
      },
      "source": [
        "[DATASET](https://www.kaggle.com/datasets/adityajn105/flickr8k?select=Images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "v6H0QvNn1RnZ"
      },
      "outputs": [],
      "source": [
        "!unzip /content/https:/www.kaggle.com/datasets/adityajn105/flickr8k/flickr8k.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "cucxT7pl5n6j"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "from keras.applications import efficientnet\n",
        "\n",
        "tf.random.set_seed(111)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "aETgo4HA6ax_"
      },
      "outputs": [],
      "source": [
        "image_dir = \"/content/Images\"\n",
        "caption_file = \"/content/captions.txt\"\n",
        "\n",
        "batch_size = 64\n",
        "image_size = (299, 299)\n",
        "vocab_size = 10000\n",
        "sequence_length = 25\n",
        "embedding_dim = 512\n",
        "ff_dim = 512"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "pOzE8QgL7JZl"
      },
      "outputs": [],
      "source": [
        "def load_caption_file(filename):\n",
        "    with open(filename) as files:\n",
        "        caption_data = files.readlines()\n",
        "        caption_mapping = {}\n",
        "        text_data = []\n",
        "        image_to_skip = set()\n",
        "\n",
        "        for line in caption_data:\n",
        "            line = line.rstrip(\"\\n\")\n",
        "            lines = line.split(\",\")\n",
        "            img_path = lines[0]\n",
        "            caption = lines[1]\n",
        "\n",
        "            img_path = os.path.join(image_dir, img_path.strip())\n",
        "\n",
        "            tokens = caption.strip().split()\n",
        "            if len(tokens) < 5 or len(tokens) > sequence_length:\n",
        "                image_to_skip.add(img_path)\n",
        "                continue\n",
        "\n",
        "            if img_path.endswith(\".jpg\") and img_path not in image_to_skip:\n",
        "                caption = \"<start> \" + caption.strip() + \" <end>\"\n",
        "                text_data.append(caption)\n",
        "\n",
        "                if img_path in caption_mapping:\n",
        "                    caption_mapping[img_path].append(caption)\n",
        "                else:\n",
        "                    caption_mapping[img_path] = [caption]\n",
        "\n",
        "        for img_path in image_to_skip:\n",
        "            if img_path in caption_mapping:\n",
        "                del caption_mapping[img_path]\n",
        "\n",
        "        return caption_mapping, text_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "5upCFtqjs8_j"
      },
      "outputs": [],
      "source": [
        "caption_mapping, text_data = load_caption_file(caption_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J133Qh-k7nx2",
        "outputId": "699796f2-9ac3-4172-85f8-f48a136eec65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total train data is ::  5616\n",
            "Total valid data is ::  1405\n"
          ]
        }
      ],
      "source": [
        "def train_val_split(caption_data, train_size = 0.8, shuffle = True):\n",
        "    all_images = list(caption_data.keys())\n",
        "\n",
        "    if shuffle:\n",
        "        np.random.shuffle(all_images)\n",
        "\n",
        "    train_split = int(len(all_images) * train_size)\n",
        "\n",
        "    train_data = {img_path : caption_data[img_path] for img_path in all_images[: train_split]}\n",
        "\n",
        "    valid_data = {img_path : caption_data[img_path] for img_path in all_images[train_split :]}\n",
        "\n",
        "    return train_data, valid_data\n",
        "\n",
        "\n",
        "train_data, valid_data = train_val_split(caption_mapping)\n",
        "print(\"Total train data is :: \", len(train_data))\n",
        "print(\"Total valid data is :: \", len(valid_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "srTV340BCdIx"
      },
      "outputs": [],
      "source": [
        "def custom_standardization(input_string):\n",
        "    lowercase = tf.strings.lower(input_string)\n",
        "    return tf.strings.regex_replace(lowercase, \"[%s]\" % re.escape(strip_chars), \"\")\n",
        "\n",
        "strip_chars = \"!\\\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\n",
        "strip_chars = strip_chars.replace(\"<\", \"\")\n",
        "strip_chars = strip_chars.replace(\">\", \"\")\n",
        "\n",
        "vectorize_layer = TextVectorization(\n",
        "    standardize = custom_standardization,\n",
        "    max_tokens = vocab_size,\n",
        "    output_sequence_length = sequence_length,\n",
        "    output_mode = \"int\")\n",
        "\n",
        "vectorize_layer.adapt(text_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Ne9RP_BAGT0P"
      },
      "outputs": [],
      "source": [
        "def load_image(img_path):\n",
        "    img = tf.io.read_file(img_path)\n",
        "    img = tf.image.decode_jpeg(img, channels = 3)\n",
        "    img = tf.image.resize(img, image_size)\n",
        "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
        "    return img\n",
        "\n",
        "\n",
        "def preprocess_data(img_path, caption):\n",
        "    img = load_image(img_path)\n",
        "    caption = vectorize_layer(caption)\n",
        "    return img, caption\n",
        "\n",
        "\n",
        "def build_dataset(images, caption):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((images, caption))\n",
        "    dataset = dataset.shuffle(batch_size * 8)\n",
        "    dataset = dataset.map(preprocess_data, num_parallel_calls = tf.data.AUTOTUNE)\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.prefetch(buffer_size = tf.data.AUTOTUNE)\n",
        "    return dataset\n",
        "\n",
        "train_dataset = build_dataset(list(train_data.keys()), list(train_data.values()))\n",
        "valid_dataset = build_dataset(list(valid_data.keys()), list(valid_data.values()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kOv9-MnW0oal",
        "outputId": "de0f29a6-5cb5-4e54-d97b-27610524876f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 299, 299, 3)\n",
            "(64, 5, 25)\n"
          ]
        }
      ],
      "source": [
        "for i, j in train_dataset.take(1):\n",
        "    print(i.shape)\n",
        "    print(j.shape)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "u7bWZCO01CLl"
      },
      "outputs": [],
      "source": [
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.RandomFlip(\"horizontal\"),\n",
        "        layers.RandomRotation(factor = 0.2),\n",
        "        layers.RandomContrast(0.3)\n",
        "    ])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building the model\n",
        "\n",
        "Our image captioning architecture consists of three models:\n",
        "\n",
        "1. A CNN: used to extract the image features\n",
        "2. A TransformerEncoder: The extracted image features are then passed to a Transformer based encoder that generates a new representation of the inputs\n",
        "3. A TransformerDecoder: This model takes the encoder output and the text data (sequences) as inputs and tries to learn to generate the caption."
      ],
      "metadata": {
        "id": "wAvGvj-7NiIN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "giJMkB_G1r97"
      },
      "outputs": [],
      "source": [
        "def feature_extractor():\n",
        "    base_model = efficientnet.EfficientNetB0(\n",
        "        input_shape = (*image_size, 3),\n",
        "        weights = \"imagenet\",\n",
        "        include_top = False)\n",
        "\n",
        "    base_model.trainable = False\n",
        "\n",
        "    base_model_output = base_model.output\n",
        "\n",
        "    base_model_output = layers.Reshape((-1, base_model_output.shape[-1]))(base_model_output)\n",
        "\n",
        "    feature_extractor = keras.Model(base_model.input, base_model_output)\n",
        "    return feature_extractor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "cb9WWvdQ5ga8"
      },
      "outputs": [],
      "source": [
        "class TransformerEncoderBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        #self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention_layer = layers.MultiHeadAttention(num_heads = num_heads, key_dim = embed_dim, dropout = 0.0)\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.dense_layer = layers.Dense(self.embed_dim, activation = \"relu\")\n",
        "\n",
        "    def call(self, inputs, training, mask = None):\n",
        "        inputs = self.layernorm_1(inputs)\n",
        "        inputs = self.dense_layer(inputs)\n",
        "        attention_output = self.attention_layer(\n",
        "            query = inputs,\n",
        "            value = inputs,\n",
        "            key = inputs,\n",
        "            attention_mask = mask,\n",
        "            training = training)\n",
        "\n",
        "        output = self.layernorm_2(inputs + attention_output)\n",
        "        return output\n",
        "\n",
        "\n",
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.sequence_length = sequence_length\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "        self.token_embed = layers.Embedding(input_dim = vocab_size, output_dim  = embed_dim)\n",
        "        self.pos_embed = layers.Embedding(input_dim = sequence_length, output_dim = embed_dim)\n",
        "        self.embed_scale = tf.math.sqrt(tf.cast(embed_dim, tf.float32))\n",
        "\n",
        "\n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(start = 0, limit = length, delta = 1)\n",
        "        embedded_tokens = self.token_embed(inputs)\n",
        "        embedded_tokens = embedded_tokens + self.embed_scale\n",
        "        embedded_positions = self.pos_embed(positions)\n",
        "        return embedded_positions + embedded_tokens\n",
        "\n",
        "    def compute_mask(self, inputs, mask = None):\n",
        "        return tf.math.not_equal(inputs, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Nogmz7OsJ6YZ"
      },
      "outputs": [],
      "source": [
        "class TransformerDecoderBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, ff_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.ff_dim = ff_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention_1 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim, dropout=0.1\n",
        "        )\n",
        "        self.attention_2 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim, dropout=0.1\n",
        "        )\n",
        "        self.ffn_layer_1 = layers.Dense(ff_dim, activation=\"relu\")\n",
        "        self.ffn_layer_2 = layers.Dense(embed_dim)\n",
        "\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.layernorm_3 = layers.LayerNormalization()\n",
        "\n",
        "        self.embedding = PositionalEmbedding(\n",
        "            embed_dim = embedding_dim,\n",
        "            sequence_length = sequence_length,\n",
        "            vocab_size = vocab_size,\n",
        "        )\n",
        "        self.out = layers.Dense(vocab_size, activation=\"softmax\")\n",
        "\n",
        "        self.dropout_1 = layers.Dropout(0.3)\n",
        "        self.dropout_2 = layers.Dropout(0.5)\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def call(self, inputs, encoder_outputs, training, mask=None):\n",
        "        inputs = self.embedding(inputs)\n",
        "        causal_mask = self.get_causal_attention_mask(inputs)\n",
        "\n",
        "        if mask is not None:\n",
        "            padding_mask = tf.cast(mask[:, :, tf.newaxis], dtype=tf.int32)\n",
        "            combined_mask = tf.cast(mask[:, tf.newaxis, :], dtype=tf.int32)\n",
        "            combined_mask = tf.minimum(combined_mask, causal_mask)\n",
        "\n",
        "        attention_output_1 = self.attention_1(\n",
        "            query=inputs,\n",
        "            value=inputs,\n",
        "            key=inputs,\n",
        "            attention_mask=combined_mask,\n",
        "            training=training,\n",
        "        )\n",
        "        out_1 = self.layernorm_1(inputs + attention_output_1)\n",
        "\n",
        "        attention_output_2 = self.attention_2(\n",
        "            query=out_1,\n",
        "            value=encoder_outputs,\n",
        "            key=encoder_outputs,\n",
        "            attention_mask=padding_mask,\n",
        "            training=training,\n",
        "        )\n",
        "        out_2 = self.layernorm_2(out_1 + attention_output_2)\n",
        "\n",
        "        ffn_out = self.ffn_layer_1(out_2)\n",
        "        ffn_out = self.dropout_1(ffn_out, training=training)\n",
        "        ffn_out = self.ffn_layer_2(ffn_out)\n",
        "\n",
        "        ffn_out = self.layernorm_3(ffn_out + out_2, training=training)\n",
        "        ffn_out = self.dropout_2(ffn_out, training=training)\n",
        "        preds = self.out(ffn_out)\n",
        "        return preds\n",
        "\n",
        "    def get_causal_attention_mask(self, inputs):\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
        "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
        "        j = tf.range(sequence_length)\n",
        "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
        "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
        "        mult = tf.concat(\n",
        "            [\n",
        "                tf.expand_dims(batch_size, -1),\n",
        "                tf.constant([1, 1], dtype=tf.int32),\n",
        "            ],\n",
        "            axis=0,\n",
        "        )\n",
        "        return tf.tile(mask, mult)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "TdkbgrmDKxo-"
      },
      "outputs": [],
      "source": [
        "class ImageCaptioningModel(keras.Model):\n",
        "    def __init__(self, base_model, encoder, decoder, num_captions_per_image=5, image_aug=None):\n",
        "        super().__init__()\n",
        "        self.base_model = base_model\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.loss_tracker = keras.metrics.Mean(name=\"loss\")\n",
        "        self.acc_tracker = keras.metrics.Mean(name=\"accuracy\")\n",
        "        self.num_captions_per_image = num_captions_per_image\n",
        "        self.image_aug = image_aug\n",
        "\n",
        "    def calculate_loss(self, y_true, y_pred, mask):\n",
        "        loss = self.loss(y_true, y_pred)\n",
        "        mask = tf.cast(mask, dtype=loss.dtype)\n",
        "        loss *= mask\n",
        "        return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
        "\n",
        "    def calculate_accuracy(self, y_true, y_pred, mask):\n",
        "        accuracy = tf.equal(y_true, tf.argmax(y_pred, axis=2))\n",
        "        accuracy = tf.math.logical_and(mask, accuracy)\n",
        "        accuracy = tf.cast(accuracy, dtype = tf.float32)\n",
        "        mask = tf.cast(mask, dtype=tf.float32)\n",
        "        return tf.reduce_sum(accuracy) / tf.reduce_sum(mask)\n",
        "\n",
        "    def _compute_caption_loss_and_acc(self, img_embed, batch_seq, training=True):\n",
        "        encoder_out = self.encoder(img_embed, training=training)\n",
        "        batch_seq_inp = batch_seq[:, :-1]\n",
        "        batch_seq_true = batch_seq[:, 1:]\n",
        "        mask = tf.math.not_equal(batch_seq_true, 0)\n",
        "        batch_seq_pred = self.decoder(\n",
        "            batch_seq_inp, encoder_out, training=training, mask=mask\n",
        "        )\n",
        "        loss = self.calculate_loss(batch_seq_true, batch_seq_pred, mask)\n",
        "        acc = self.calculate_accuracy(batch_seq_true, batch_seq_pred, mask)\n",
        "        return loss, acc\n",
        "\n",
        "    def train_step(self, batch_data):\n",
        "        batch_img, batch_seq = batch_data\n",
        "        batch_loss = 0\n",
        "        batch_acc = 0\n",
        "\n",
        "        if self.image_aug:\n",
        "            batch_img = self.image_aug(batch_img)\n",
        "\n",
        "        # 1. Get image embeddings\n",
        "        img_embed = self.base_model(batch_img)\n",
        "\n",
        "        # 2. Pass each of the five captions one by one to the decoder\n",
        "        # along with the encoder outputs and compute the loss as well as accuracy\n",
        "        # for each caption.\n",
        "        for i in range(self.num_captions_per_image):\n",
        "            with tf.GradientTape() as tape:\n",
        "                loss, acc = self._compute_caption_loss_and_acc(\n",
        "                    img_embed, batch_seq[:, i, :], training=True\n",
        "                )\n",
        "\n",
        "                # 3. Update loss and accuracy\n",
        "                batch_loss += loss\n",
        "                batch_acc += acc\n",
        "\n",
        "            # 4. Get the list of all the trainable weights\n",
        "            train_vars = (\n",
        "                self.encoder.trainable_variables + self.decoder.trainable_variables\n",
        "            )\n",
        "\n",
        "            # 5. Get the gradients\n",
        "            grads = tape.gradient(loss, train_vars)\n",
        "\n",
        "            # 6. Update the trainable weights\n",
        "            self.optimizer.apply_gradients(zip(grads, train_vars))\n",
        "\n",
        "        # 7. Update the trackers\n",
        "        batch_acc /= float(self.num_captions_per_image)\n",
        "        self.loss_tracker.update_state(batch_loss)\n",
        "        self.acc_tracker.update_state(batch_acc)\n",
        "\n",
        "        # 8. Return the loss and accuracy values\n",
        "        return {\n",
        "            \"loss\": self.loss_tracker.result(),\n",
        "            \"acc\": self.acc_tracker.result(),\n",
        "        }\n",
        "\n",
        "    def test_step(self, batch_data):\n",
        "        batch_img, batch_seq = batch_data\n",
        "        batch_loss = 0\n",
        "        batch_acc = 0\n",
        "\n",
        "        # 1. Get image embeddings\n",
        "        img_embed = self.base_model(batch_img)\n",
        "\n",
        "        # 2. Pass each of the five captions one by one to the decoder\n",
        "        # along with the encoder outputs and compute the loss as well as accuracy\n",
        "        # for each caption.\n",
        "        for i in range(self.num_captions_per_image):\n",
        "            loss, acc = self._compute_caption_loss_and_acc(\n",
        "                img_embed, batch_seq[:, i, :], training=False\n",
        "            )\n",
        "\n",
        "            # 3. Update batch loss and batch accuracy\n",
        "            batch_loss += loss\n",
        "            batch_acc += acc\n",
        "\n",
        "        batch_acc /= float(self.num_captions_per_image)\n",
        "\n",
        "        # 4. Update the trackers\n",
        "        self.loss_tracker.update_state(batch_loss)\n",
        "        self.acc_tracker.update_state(batch_acc)\n",
        "\n",
        "        # 5. Return the loss and accuracy values\n",
        "        return {\n",
        "            \"loss\": self.loss_tracker.result(),\n",
        "            \"acc\": self.acc_tracker.result(),\n",
        "        }\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        # We need to list our metrics here so the `reset_states()` can be\n",
        "        # called automatically.\n",
        "        return [self.loss_tracker, self.acc_tracker]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "tiO2fjzNNDiy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97a5ffd6-0dbc-4109-d65a-850f3edbf653"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n",
            "16705208/16705208 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "base_model = feature_extractor()\n",
        "encoder = TransformerEncoderBlock(embed_dim = embedding_dim, num_heads = 1)\n",
        "decoder = TransformerDecoderBlock(embed_dim = embedding_dim, ff_dim = ff_dim, num_heads = 2)\n",
        "captioning_model = ImageCaptioningModel(\n",
        "    base_model = base_model,\n",
        "    encoder = encoder,\n",
        "    decoder = decoder,\n",
        "    image_aug = data_augmentation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "tOz7JCXQPHIQ"
      },
      "outputs": [],
      "source": [
        "class LRSchedule(keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, post_warmup_learning_rate, warmup_steps):\n",
        "        super().__init__()\n",
        "        self.post_warmup_learning_rate = post_warmup_learning_rate\n",
        "        self.warmup_steps = warmup_steps\n",
        "\n",
        "    def __call__(self, step):\n",
        "        global_step = tf.cast(step, tf.float32)\n",
        "        warmup_steps = tf.cast(self.warmup_steps, tf.float32)\n",
        "        warmup_progress = global_step / warmup_steps\n",
        "        warmup_learning_rate = self.post_warmup_learning_rate * warmup_progress\n",
        "        return tf.cond(\n",
        "            global_step < warmup_steps,\n",
        "            lambda: warmup_learning_rate,\n",
        "            lambda: self.post_warmup_learning_rate,\n",
        "        )\n",
        "\n",
        "num_train_step = len(train_dataset) * 30\n",
        "num_warmup_steps = num_train_step // 15\n",
        "lr_schedule = LRSchedule(post_warmup_learning_rate=1e-4, warmup_steps=num_warmup_steps)\n",
        "\n",
        "captioning_model.compile(\n",
        "    optimizer = keras.optimizers.Adam(learning_rate = lr_schedule),\n",
        "    loss = keras.losses.SparseCategoricalCrossentropy(from_logits = False, reduction = \"none\")\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XP0PcdqzR4P9"
      },
      "outputs": [],
      "source": [
        "early_stopping_cb = keras.callbacks.EarlyStopping(monitor = \"val_loss\", patience = 3, restore_best_weights = True)\n",
        "\n",
        "captioning_model.fit(\n",
        "    train_dataset,\n",
        "    validation_data = valid_dataset,\n",
        "    epochs = 50,\n",
        "    callbacks = [early_stopping_cb])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = vectorize_layer.get_vocabulary()\n",
        "index_lookup = dict(zip(range(len(vocab)), vocab))\n",
        "max_decoded_sentence_length = sequence_length - 1\n",
        "valid_images = list(valid_data.keys())\n",
        "\n",
        "\n",
        "def generate_caption():\n",
        "    # Select a random image from the validation dataset\n",
        "    sample_img = np.random.choice(valid_images)\n",
        "\n",
        "    # Read the image from the disk\n",
        "    sample_img = load_image(sample_img)\n",
        "    img = sample_img.numpy().clip(0, 255).astype(np.uint8)\n",
        "    plt.imshow(img)\n",
        "    plt.show()\n",
        "\n",
        "    # Pass the image to the CNN\n",
        "    img = tf.expand_dims(sample_img, 0)\n",
        "    img = captioning_model.base_model(img)\n",
        "\n",
        "    # Pass the image features to the Transformer encoder\n",
        "    encoded_img = captioning_model.encoder(img, training=False)\n",
        "\n",
        "    # Generate the caption using the Transformer decoder\n",
        "    decoded_caption = \"<start> \"\n",
        "    for i in range(max_decoded_sentence_length):\n",
        "        tokenized_caption = vectorize_layer([decoded_caption])[:, :-1]\n",
        "        mask = tf.math.not_equal(tokenized_caption, 0)\n",
        "        predictions = captioning_model.decoder(\n",
        "            tokenized_caption, encoded_img, training=False, mask=mask\n",
        "        )\n",
        "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
        "        sampled_token = index_lookup[sampled_token_index]\n",
        "        if sampled_token == \"<end>\":\n",
        "            break\n",
        "        decoded_caption += \" \" + sampled_token\n",
        "\n",
        "    decoded_caption = decoded_caption.replace(\"<start> \", \"\")\n",
        "    decoded_caption = decoded_caption.replace(\" <end>\", \"\").strip()\n",
        "    print(\"Predicted Caption: \", decoded_caption)\n",
        "\n",
        "\n",
        "# Check predictions for a few samples\n",
        "generate_caption()\n",
        "generate_caption()\n",
        "generate_caption()"
      ],
      "metadata": {
        "id": "HnGOBzZN8Lnz"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOPZgG5ej91W2ESdV7yggMO",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}